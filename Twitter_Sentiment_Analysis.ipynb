{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Twitter Sentiment Analysis**\n",
        "\n",
        "data source: [kaggle](https://www.kaggle.com/code/stoicstatic/twitter-sentiment-analysis-for-beginners/data)\n",
        "\n",
        "The dataset being used is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = Negative, 4 = Positive) and they can be used to detect sentiment.\n",
        "\n",
        "It contains the following 6 fields:\n",
        "\n",
        "sentiment: the polarity of the tweet (0 = negative, 4 = positive)\n",
        "ids: The id of the tweet (2087)\n",
        "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
        "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
        "user: the user that tweeted (robotickilldozr)\n",
        "text: the text of the tweet (Lyx is cool)\n",
        "We require only the sentiment and text fields, so we discard the rest.\n",
        "\n",
        "Furthermore, we're changing the sentiment field so that it has new values to reflect the sentiment. (0 = Negative, 1 = Positive)"
      ],
      "metadata": {
        "id": "5AzPdewSwFOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import and setup stuff**"
      ],
      "metadata": {
        "id": "aOqAFZZ8ZQE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utilities\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle \n",
        "\n",
        "#plotting\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#nltk\n",
        "import os\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#sklearn\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "k7CXilciSB9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing dataset**"
      ],
      "metadata": {
        "id": "NdjfJy4CEJw_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWlN4oD4FXrz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111fd5c8-023b-4a2d-dbbf-f9d36f998b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Running this cell will provide you with a token to link your drive to this notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle('/content/drive/MyDrive/Fix upload github/Sentiment-BNB.pkl')\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "mWgdY6GFUWeB",
        "outputId": "18609387-1666-4c7b-88b2-18f7f47cbf5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b1db9a850c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Fix upload github/Sentiment-BNB.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'BernoulliNB' object has no attribute 'info'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_pickle('/content/drive/MyDrive/Fix upload github/Sentiment-BNB.pkl')\n",
        "df\n",
        "#with open(\"/content/drive/MyDrive/Fix upload github/Sentiment-BNB.pickle\", \"rb\") as f:\n",
        " \n",
        " #   object = pkl.load(f)\n",
        "    \n",
        "#df.to_csv(r'file.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCX1b2OQO7OW",
        "outputId": "478c3542-0261-454f-bfe7-533df0e7532a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BernoulliNB(alpha=2)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lv1Ayf1qGrI6",
        "outputId": "bc07675c-370d-4d2c-b4b6-f537b6ecee71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 74682 entries, 0 to 74681\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Text    73996 non-null  object\n",
            " 1   Target  74682 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 1.1+ MB\n",
            "None\n",
            "                                                Text    Target\n",
            "0  im getting on borderlands and i will murder yo...  Positive\n",
            "1  I am coming to the borders and I will kill you...  Positive\n",
            "2  im getting on borderlands and i will kill you ...  Positive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74682, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Checkout the labels of our data\n",
        "labels_csv = pd.read_csv(\"drive/My Drive/Twitter/twitter_training.csv\",\n",
        "                         names=[\"Id\",\"Entity\",\"Target\",\"Text\"],header=None)\n",
        "# Deleting Entity and ID \n",
        "labels_csv = labels_csv.drop(['Entity','Id'], axis=1)\n",
        "\n",
        "# Swicthing text and target position\n",
        "data = labels_csv[['Text','Target']]\n",
        "\n",
        "# Getting info from labels_csv\n",
        "print(data.info())\n",
        "\n",
        "# print 3 upper of label_csv data\n",
        "print(data.head(3))\n",
        "\n",
        "# Check the dimension\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# examining closer, we find there are 4909 duplicate rows\n",
        "np.sum(data.duplicated())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxiRWoo64uSf",
        "outputId": "2cd1cff0-7086-414c-83d0-9922a2757d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4909"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's drop the duplicates\n",
        "df = data.drop_duplicates()\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne94ZS1p5pt_",
        "outputId": "ced25afd-5dcb-4b48-f022-6b8f1295bb1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69773, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Target'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGCYN61p4_w7",
        "outputId": "31ea709b-88b4-46e1-9ba7-d69af362464b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Negative      21238\n",
              "Positive      19139\n",
              "Neutral       17111\n",
              "Irrelevant    12285\n",
              "Name: Target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for completeness of data\n",
        "\n",
        "print(f\"{np.sum(df['Text'].isna())} rows have no Text\")\n",
        "print(f\"{np.sum(df['Target'].isna())} rows have no Target\")\n",
        "print(f\"{np.sum(df['Sentiment'].isna())} rows have no Sentiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qkx-ug9BMcq",
        "outputId": "b13ff040-aedc-4827-e10c-3fa19bc187d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 rows have no Text\n",
            "0 rows have no Target\n",
            "0 rows have no Sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labeling the target, -1 = Negative, 0 = Neutral and irrelevant, 1 = Positive \n",
        "sentiment = []\n",
        "\n",
        "for i in df[\"Target\"]:\n",
        "    if i == \"Positive\":\n",
        "        sentiment.append(1)\n",
        "    elif (i == \"Irrelevant\") or (i == \"Neutral\"):\n",
        "        sentiment.append(0)\n",
        "    else:\n",
        "        sentiment.append(-1)\n",
        "df[\"Sentiment\"] = sentiment"
      ],
      "metadata": {
        "id": "ASbpo7Et6y0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1911f6f-618f-465d-8140-88f106ed1cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "tOskca9O6x3z",
        "outputId": "cf8b3848-024c-429b-a016-11b3b64c23c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text    Target  Sentiment\n",
              "0  im getting on borderlands and i will murder yo...  Positive          1\n",
              "1  I am coming to the borders and I will kill you...  Positive          1\n",
              "2  im getting on borderlands and i will kill you ...  Positive          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55061cff-1258-40df-9272-4a543e1458c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Target</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>im getting on borderlands and i will murder yo...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I am coming to the borders and I will kill you...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>im getting on borderlands and i will kill you ...</td>\n",
              "      <td>Positive</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55061cff-1258-40df-9272-4a543e1458c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-55061cff-1258-40df-9272-4a543e1458c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-55061cff-1258-40df-9272-4a543e1458c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set (stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "GR0qMLkKVa5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaner**"
      ],
      "metadata": {
        "id": "eOSNNvV2VP0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Text\"] = df[\"Text\"].str.replace(\"\\d\",\"\")"
      ],
      "metadata": {
        "id": "S5g0qSN_EGXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(data):\n",
        "    # Tokens\n",
        "    tokens = word_tokenize(str(data).replace(\"'\", \"\").lower()) \n",
        "    \n",
        "    # Remove Puncs\n",
        "    without_punc = [w for w in tokens if w.isalpha()]\n",
        "    \n",
        "    # Stopwords\n",
        "    without_sw = [t for t in without_punc if t not in stop_words]\n",
        "    \n",
        "    # Lemmatize\n",
        "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_sw]\n",
        "    # Stem\n",
        "    text_cleaned = [PorterStemmer().stem(w) for w in text_len]\n",
        "    \n",
        "    return \" \".join(text_cleaned)"
      ],
      "metadata": {
        "id": "zSzB4Mt0EJJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-xMXqN1WHpF",
        "outputId": "82a3081a-d87f-4f2f-d91a-ce1b3793142c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Text\"] = df[\"Text\"].apply(cleaner)\n",
        "df[\"Text\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVmna4SAVoZ1",
        "outputId": "93ebcf58-9e2e-4ac4-9ab6-783cfb0f812c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     im get borderland murder\n",
              "1             come border kill\n",
              "2       im get borderland kill\n",
              "3    im come borderland murder\n",
              "4     im get borderland murder\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Text\"] = df[\"Text\"].str.replace(\"im\",\" \")\n",
        "df[\"Text\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq1KceNEZBtf",
        "outputId": "a8801d50-2b69-4b7d-aa33-08f7e1a873e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       get borderland murder\n",
              "1            come border kill\n",
              "2         get borderland kill\n",
              "3      come borderland murder\n",
              "4       get borderland murder\n",
              "Name: Text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP with `spaCy` library**"
      ],
      "metadata": {
        "id": "h96taMLqCrs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download spaCy model for American English\n",
        "!python3 -m spacy download en_core_web_sm\n",
        "import spacy \n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "-DDZIKnFDDgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "IFCSSebqYzy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to also keep #hashtags as a token, so we will modify the spaCy model's token_match\n",
        "import re\n",
        "# Retrieve the default token-matching regex pattern\n",
        "re_token_match = spacy.tokenizer._get_regex_pattern(nlp.Defaults.token_match)\n",
        "# Add #hashtag pattern\n",
        "re_token_match = f\"({re_token_match}|#\\\\w+)\"\n",
        "nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
        "# Now let's try again\n",
        "s = \"2020 can't get any worse #ihate2020 @bestfriend <https://t.co>\"\n",
        "doc = nlp(s)\n",
        "# Let's look at the lemmas and is stopword of each token\n",
        "print(f\"Token\\\\t\\\\tLemma\\\\t\\\\tStopword\")\n",
        "print(\"=\"*40)\n",
        "for token in doc:\n",
        "    print(f\"{token}\\\\t\\\\t{token.lemma_}\\\\t\\\\t{token.is_stop}\")"
      ],
      "metadata": {
        "id": "YGQW1jSiDzYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign comments\n",
        "comments = data.loc[0, 'Text']\n",
        "print(comments)"
      ],
      "metadata": {
        "id": "PzzyDWeZm6NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "[sent.text for sent in nlp(comments).sents]"
      ],
      "metadata": {
        "id": "TvTEP4iOq7-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit -n 10\n",
        "# SpaCy with DependencyParser\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "data.loc[:5000, \"Text\"].apply(lambda x: [sent.text for sent in nlp(x).se"
      ],
      "metadata": {
        "id": "WaURefjqy4fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "nlp_en = English()"
      ],
      "metadata": {
        "id": "kP1OvUiFyZ4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Text'] = data['Text'].apply(lambda x: [sent.text for sent in nlp(x).sents])"
      ],
      "metadata": {
        "id": "NIdxLmQ0wi_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  text = text.str.replace(\"\\n\", \" \")\n",
        "  return text\n",
        "\n",
        "data['Text'] = preprocess(data['Text'])"
      ],
      "metadata": {
        "id": "SQUcTFWfYNe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "ND78QreLOU7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'sentences': ['This is a very good site. I will recommend it to others.', 'Can you please give me a call at 9983938428. have issues with the listings.', 'good work! keep it up']})\n",
        "df"
      ],
      "metadata": {
        "id": "0hdXLYZ6XX4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "df"
      ],
      "metadata": {
        "id": "QrB0w1DNezG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nojJWdiE2dWa"
      },
      "outputs": [],
      "source": [
        "def clean_file(text):\n",
        "    text = text.lower()\n",
        "    return text.replace(\"\\n\",\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['Text'].apply(clean_file)\n",
        "x "
      ],
      "metadata": {
        "id": "_lczDpxXO65c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(main):\n",
        "    # Tokens\n",
        "    tokens = word_tokenize(str(main).replace(\"'\", \"\").lower()) \n",
        "    \n",
        "    # Remove Puncs\n",
        "    without_punc = [w for w in tokens if w.isalpha()]\n",
        "    \n",
        "    # Stopwords\n",
        "    without_sw = [t for t in without_punc if t not in stop_words]\n",
        "    \n",
        "    # Lemmatize\n",
        "    text_len = [WordNetLemmatizer().lemmatize(t) for t in without_sw]\n",
        "    # Stem\n",
        "    text_cleaned = [PorterStemmer().stem(w) for w in text_len]\n",
        "    \n",
        "    return \" \".join(text_cleaned)"
      ],
      "metadata": {
        "id": "JCM1pMqPU8gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQXdumB5aq3G"
      },
      "outputs": [],
      "source": [
        "x=data['Text']\n",
        "y=data['Target']\n",
        "x=x.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HE9qYh_VD9W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LW7ehr0tWJ6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2_P5oQDHzpFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d7q8Iqgqy5qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Yp9NUJQvKXl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}